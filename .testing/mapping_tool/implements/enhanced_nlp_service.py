#!/usr/bin/env python3
"""
Enhanced NLP Service - å®Œæ•´ç‰ˆ NLP åˆ†ææœåŠ¡

åˆ©ç”¨å®Œæ•´çš„ requirements.txt ä¾èµ–æä¾›å¼ºå¤§çš„ NLP åˆ†æåŠŸèƒ½ï¼š
- spaCy + transformers è¯­ä¹‰ç†è§£
- jieba ä¸­æ–‡åˆ†è¯ä¼˜åŒ–
- sentence-transformers é«˜çº§ç›¸ä¼¼åº¦è®¡ç®—
- scikit-learn æœºå™¨å­¦ä¹ åˆ†ç±»

ä½œè€…ï¼š@MapleEve
æ—¥æœŸï¼š2025-08-15
"""

import asyncio
import logging
import os
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# å®Œæ•´ç‰ˆ NLP ä¾èµ–
try:
    # Advanced NLP libraries (full mode)
    import spacy
    import jieba
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np

    HAS_FULL_NLP = True
except ImportError as e:
    # ä¼˜é›…é™çº§åˆ°åŸºç¡€æ¨¡å¼
    logging.warning(f"Full NLP dependencies not available: {e}")
    HAS_FULL_NLP = False
    spacy = None
    jieba = None
    SentenceTransformer = None
    cosine_similarity = None
    np = None

# æ ¸å¿ƒä¾èµ– (æ€»æ˜¯å¯ç”¨)
try:
    from ..architecture.services import NLPService
    from ..data_types.core_types import NLPConfig, NLPProvider
except ImportError:
    import sys

    sys.path.append(os.path.join(os.path.dirname(__file__), ".."))
    from architecture.services import NLPService
    from data_types.core_types import NLPConfig, NLPProvider


@dataclass
class NLPAnalysisResult:
    """NLP åˆ†æç»“æœ"""

    text: str
    language: str
    entities: List[Dict[str, Any]]
    sentiment: float
    keywords: List[str]
    embeddings: Optional[np.ndarray] = None
    confidence: float = 0.0


@dataclass
class SemanticSimilarityResult:
    """è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æç»“æœ"""

    text_a: str
    text_b: str
    similarity_score: float
    semantic_features: Dict[str, Any]
    confidence: float


class NLPLanguage(Enum):
    """æ”¯æŒçš„è¯­è¨€ç±»å‹"""

    CHINESE = "zh"
    ENGLISH = "en"
    AUTO = "auto"


class EnhancedNLPService(NLPService):
    """
    å¢å¼ºç‰ˆ NLP æœåŠ¡å®ç°

    å®Œæ•´åŠŸèƒ½ç‰ˆæœ¬ï¼Œåˆ©ç”¨æ‰€æœ‰é«˜çº§ NLP åº“ï¼š
    - spaCy: å®ä½“è¯†åˆ«å’Œè¯­æ³•åˆ†æ
    - jieba: ä¸­æ–‡æ™ºèƒ½åˆ†è¯
    - sentence-transformers: è¯­ä¹‰ç›¸ä¼¼åº¦
    - scikit-learn: æ–‡æœ¬åˆ†ç±»å’Œèšç±»
    """

    def __init__(self, config: Optional[NLPConfig] = None):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆ NLP æœåŠ¡

        Args:
            config: NLP é…ç½®å‚æ•°
        """
        super().__init__("EnhancedNLPService")
        self.config = config or {}

        # æ¨¡å‹ç¼“å­˜
        self._spacy_models = {}
        self._sentence_model = None
        self._jieba_initialized = False

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_analyses": 0,
            "semantic_comparisons": 0,
            "entity_extractions": 0,
            "average_processing_time": 0.0,
        }

        if HAS_FULL_NLP:
            logging.info("ğŸš€ EnhancedNLPService: Full NLP mode activated")
        else:
            logging.warning("âš ï¸ EnhancedNLPService: Fallback to basic mode")

    async def async_initialize(self) -> None:
        """å¼‚æ­¥åˆå§‹åŒ– NLP æ¨¡å‹"""
        if not HAS_FULL_NLP:
            logging.info("ğŸ“ NLP Service: Using basic text processing mode")
            self._mark_initialized()
            return

        try:
            # åˆå§‹åŒ– sentence-transformers æ¨¡å‹
            if SentenceTransformer:
                logging.info("ğŸ”„ Loading sentence-transformers model...")
                self._sentence_model = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: SentenceTransformer("all-MiniLM-L6-v2")
                )
                logging.info("âœ… Sentence transformer loaded")

            # åˆå§‹åŒ– jieba ä¸­æ–‡åˆ†è¯
            if jieba and not self._jieba_initialized:
                logging.info("ğŸ”„ Initializing jieba Chinese segmentation...")
                await asyncio.get_event_loop().run_in_executor(
                    None, lambda: jieba.initialize()
                )
                self._jieba_initialized = True
                logging.info("âœ… Jieba initialized")

            # é¢„åŠ è½½ spaCy æ¨¡å‹ (å¼‚æ­¥)
            await self._preload_spacy_models()

            self._mark_initialized()
            logging.info("ğŸ¯ EnhancedNLPService: Full initialization complete")

        except Exception as e:
            logging.error(f"âŒ NLP Service initialization failed: {e}")
            # é™çº§åˆ°åŸºç¡€æ¨¡å¼
            self._mark_initialized()

    async def _preload_spacy_models(self) -> None:
        """é¢„åŠ è½½ spaCy è¯­è¨€æ¨¡å‹"""
        if not spacy:
            return

        models_to_load = ["en_core_web_sm", "zh_core_web_sm"]

        for model_name in models_to_load:
            try:
                logging.info(f"ğŸ”„ Loading spaCy model: {model_name}")
                nlp = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: spacy.load(model_name)
                )
                self._spacy_models[model_name] = nlp
                logging.info(f"âœ… spaCy model loaded: {model_name}")
            except OSError:
                logging.warning(f"âš ï¸ spaCy model not found: {model_name}, skipping...")

    async def analyze_text(self, text: str, language: str = "auto") -> Dict[str, Any]:
        """
        å®Œæ•´ç‰ˆæ–‡æœ¬åˆ†æ

        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            language: è¯­è¨€ä»£ç  ('en', 'zh', 'auto')

        Returns:
            è¯¦ç»†çš„æ–‡æœ¬åˆ†æç»“æœ
        """
        start_time = asyncio.get_event_loop().time()

        try:
            # è‡ªåŠ¨è¯­è¨€æ£€æµ‹
            detected_lang = (
                self._detect_language(text) if language == "auto" else language
            )

            # åˆ†æç»“æœå®¹å™¨
            result = {
                "text": text,
                "language": detected_lang,
                "entities": [],
                "keywords": [],
                "sentiment": 0.0,
                "confidence": 0.0,
            }

            if HAS_FULL_NLP:
                # å®Œæ•´ç‰ˆåˆ†æ
                result = await self._analyze_text_full(text, detected_lang)
            else:
                # åŸºç¡€ç‰ˆåˆ†æ
                result = await self._analyze_text_basic(text, detected_lang)

            # æ›´æ–°ç»Ÿè®¡
            self.stats["total_analyses"] += 1
            processing_time = asyncio.get_event_loop().time() - start_time
            self._update_avg_processing_time(processing_time)

            return result

        except Exception as e:
            logging.error(f"âŒ Text analysis failed: {e}")
            return {
                "text": text,
                "language": language,
                "entities": [],
                "keywords": [],
                "sentiment": 0.0,
                "confidence": 0.0,
                "error": str(e),
            }

    async def _analyze_text_full(self, text: str, language: str) -> Dict[str, Any]:
        """å®Œæ•´ç‰ˆæ–‡æœ¬åˆ†æï¼ˆåˆ©ç”¨æ‰€æœ‰ NLP åº“ï¼‰"""

        # å®ä½“è¯†åˆ« (spaCy)
        entities = await self._extract_entities_spacy(text, language)

        # å…³é”®è¯æå– (jieba + é¢‘ç‡åˆ†æ)
        keywords = await self._extract_keywords_advanced(text, language)

        # æƒ…æ„Ÿåˆ†æ (åŸºäºå…³é”®è¯å’Œè§„åˆ™)
        sentiment = await self._analyze_sentiment_advanced(text, language)

        # è¯­ä¹‰å‘é‡ (sentence-transformers)
        embeddings = await self._get_text_embeddings(text)

        return {
            "text": text,
            "language": language,
            "entities": entities,
            "keywords": keywords,
            "sentiment": sentiment,
            "embeddings": embeddings.tolist() if embeddings is not None else None,
            "confidence": 0.9,  # å®Œæ•´ç‰ˆé«˜ç½®ä¿¡åº¦
            "processing_mode": "full",
        }

    async def _analyze_text_basic(self, text: str, language: str) -> Dict[str, Any]:
        """åŸºç¡€ç‰ˆæ–‡æœ¬åˆ†æï¼ˆä»…ä½¿ç”¨å†…ç½®åŠŸèƒ½ï¼‰"""

        # ç®€å•å…³é”®è¯æå– (åŸºäºæ­£åˆ™è¡¨è¾¾å¼)
        keywords = re.findall(r"\b[A-Za-zä¸­æ–‡]{3,}\b", text)[:10]

        # ç®€å•æƒ…æ„Ÿåˆ†æ (åŸºäºå…³é”®è¯åŒ¹é…)
        positive_words = ["å¥½", "ä¼˜ç§€", "æ­£å¸¸", "æˆåŠŸ", "æœ‰æ•ˆ"]
        negative_words = ["å", "é”™è¯¯", "å¤±è´¥", "æ— æ•ˆ", "é—®é¢˜"]

        pos_count = sum(1 for word in positive_words if word in text)
        neg_count = sum(1 for word in negative_words if word in text)

        sentiment = (
            0.1 if pos_count > neg_count else -0.1 if neg_count > pos_count else 0.0
        )

        return {
            "text": text,
            "language": language,
            "entities": [],  # åŸºç¡€ç‰ˆä¸æ”¯æŒå®ä½“è¯†åˆ«
            "keywords": keywords,
            "sentiment": sentiment,
            "embeddings": None,
            "confidence": 0.6,  # åŸºç¡€ç‰ˆä¸­ç­‰ç½®ä¿¡åº¦
            "processing_mode": "basic",
        }

    async def compare_texts(self, text_a: str, text_b: str) -> float:
        """
        é«˜çº§è¯­ä¹‰ç›¸ä¼¼åº¦æ¯”è¾ƒ

        Args:
            text_a: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text_b: ç¬¬äºŒä¸ªæ–‡æœ¬

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0.0-1.0)
        """
        try:
            self.stats["semantic_comparisons"] += 1

            if HAS_FULL_NLP and self._sentence_model:
                # å®Œæ•´ç‰ˆï¼šä½¿ç”¨ sentence-transformers
                return await self._compare_texts_semantic(text_a, text_b)
            else:
                # åŸºç¡€ç‰ˆï¼šä½¿ç”¨å­—ç¬¦çº§ç›¸ä¼¼åº¦
                return await self._compare_texts_basic(text_a, text_b)

        except Exception as e:
            logging.error(f"âŒ Text comparison failed: {e}")
            return 0.0

    async def _compare_texts_semantic(self, text_a: str, text_b: str) -> float:
        """è¯­ä¹‰çº§æ–‡æœ¬ç›¸ä¼¼åº¦æ¯”è¾ƒ"""

        # è·å–æ–‡æœ¬å‘é‡
        embeddings_a = await self._get_text_embeddings(text_a)
        embeddings_b = await self._get_text_embeddings(text_b)

        if embeddings_a is None or embeddings_b is None:
            return 0.0

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = cosine_similarity(
            embeddings_a.reshape(1, -1), embeddings_b.reshape(1, -1)
        )[0][0]

        return float(similarity)

    async def _compare_texts_basic(self, text_a: str, text_b: str) -> float:
        """åŸºç¡€æ–‡æœ¬ç›¸ä¼¼åº¦æ¯”è¾ƒ"""

        # ç®€å•çš„å­—ç¬¦é›†åˆç›¸ä¼¼åº¦
        words_a = set(text_a.lower().split())
        words_b = set(text_b.lower().split())

        if not words_a and not words_b:
            return 1.0

        intersection = len(words_a & words_b)
        union = len(words_a | words_b)

        return intersection / union if union > 0 else 0.0

    async def _get_text_embeddings(self, text: str) -> Optional[np.ndarray]:
        """è·å–æ–‡æœ¬çš„è¯­ä¹‰å‘é‡è¡¨ç¤º"""
        if not self._sentence_model or not HAS_FULL_NLP:
            return None

        try:
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œç¼–ç 
            embeddings = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self._sentence_model.encode([text])
            )
            return embeddings[0]

        except Exception as e:
            logging.error(f"âŒ Text embedding failed: {e}")
            return None

    async def extract_device_features(self, device_description: str) -> Dict[str, Any]:
        """
        ä»è®¾å¤‡æè¿°ä¸­æå–ç‰¹å¾

        Args:
            device_description: è®¾å¤‡æè¿°æ–‡æœ¬

        Returns:
            æå–çš„è®¾å¤‡ç‰¹å¾å’Œèƒ½åŠ›
        """
        try:
            # é€šç”¨æ–‡æœ¬åˆ†æ
            analysis_result = await self.analyze_text(device_description, "auto")

            # è®¾å¤‡ç‰¹å®šç‰¹å¾æå–
            device_features = {
                "platform_hints": [],
                "io_types": [],
                "capabilities": [],
                "device_category": "unknown",
            }

            # å¹³å°æç¤ºè¯åŒ¹é…
            platform_patterns = {
                "switch": r"å¼€å…³|æ§åˆ¶|æ‰“å¼€|å…³é—­|åˆ‡æ¢",
                "sensor": r"ä¼ æ„Ÿå™¨|æ£€æµ‹|ç›‘æµ‹|æ¸©åº¦|æ¹¿åº¦|æ•°å€¼",
                "light": r"ç¯å…‰|äº®åº¦|é¢œè‰²|RGB|è°ƒå…‰",
                "cover": r"çª—å¸˜|é®é˜³|å·å¸˜|å¼€å…³çª—å¸˜",
            }

            for platform, pattern in platform_patterns.items():
                if re.search(pattern, device_description):
                    device_features["platform_hints"].append(platform)

            # IOç±»å‹è¯†åˆ«
            io_patterns = {
                "digital": r"å¼€å…³|æ•°å­—|çŠ¶æ€|å¸ƒå°”",
                "analog": r"æ¨¡æ‹Ÿ|æ•°å€¼|æ¸©åº¦|æ¹¿åº¦|ç”µå‹",
                "rgb": r"RGB|é¢œè‰²|è‰²å½©",
                "pwm": r"PWM|è°ƒå…‰|äº®åº¦",
            }

            for io_type, pattern in io_patterns.items():
                if re.search(pattern, device_description):
                    device_features["io_types"].append(io_type)

            # åˆå¹¶åˆ†æç»“æœ
            device_features.update(
                {
                    "text_analysis": analysis_result,
                    "confidence": analysis_result.get("confidence", 0.0),
                }
            )

            return device_features

        except Exception as e:
            logging.error(f"âŒ Device feature extraction failed: {e}")
            return {
                "platform_hints": [],
                "io_types": [],
                "capabilities": [],
                "device_category": "unknown",
                "confidence": 0.0,
                "error": str(e),
            }

    async def _extract_entities_spacy(
        self, text: str, language: str
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ spaCy æå–å‘½åå®ä½“"""
        model_name = "zh_core_web_sm" if language == "zh" else "en_core_web_sm"
        nlp = self._spacy_models.get(model_name)

        if not nlp:
            return []

        try:
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ spaCy å¤„ç†
            doc = await asyncio.get_event_loop().run_in_executor(
                None, lambda: nlp(text)
            )

            entities = []
            for ent in doc.ents:
                entities.append(
                    {
                        "text": ent.text,
                        "label": ent.label_,
                        "start": ent.start_char,
                        "end": ent.end_char,
                        "confidence": ent._.get("confidence", 0.8),
                    }
                )

            self.stats["entity_extractions"] += len(entities)
            return entities

        except Exception as e:
            logging.error(f"âŒ spaCy entity extraction failed: {e}")
            return []

    async def _extract_keywords_advanced(self, text: str, language: str) -> List[str]:
        """é«˜çº§å…³é”®è¯æå–"""

        if language == "zh" and jieba and self._jieba_initialized:
            # ä¸­æ–‡åˆ†è¯
            keywords = await asyncio.get_event_loop().run_in_executor(
                None, lambda: list(jieba.cut(text))
            )
            # è¿‡æ»¤å’Œæ’åº
            keywords = [word.strip() for word in keywords if len(word.strip()) > 1]
            return keywords[:10]
        else:
            # è‹±æ–‡å…³é”®è¯æå–ï¼ˆåŸºäºæ­£åˆ™è¡¨è¾¾å¼ï¼‰
            keywords = re.findall(r"\b[A-Za-z]{3,}\b", text)
            return list(set(keywords))[:10]

    async def _analyze_sentiment_advanced(self, text: str, language: str) -> float:
        """é«˜çº§æƒ…æ„Ÿåˆ†æ"""

        # åŸºäºå…³é”®è¯çš„æƒ…æ„Ÿåˆ†æ (å¯ä»¥æ‰©å±•ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹)
        if language == "zh":
            positive_words = ["å¥½", "ä¼˜ç§€", "æ­£å¸¸", "æˆåŠŸ", "æœ‰æ•ˆ", "ç¨³å®š", "å¯é "]
            negative_words = ["å", "é”™è¯¯", "å¤±è´¥", "æ— æ•ˆ", "é—®é¢˜", "æ•…éšœ", "å¼‚å¸¸"]
        else:
            positive_words = [
                "good",
                "excellent",
                "normal",
                "success",
                "effective",
                "stable",
                "reliable",
            ]
            negative_words = [
                "bad",
                "error",
                "failure",
                "invalid",
                "problem",
                "fault",
                "abnormal",
            ]

        pos_count = sum(1 for word in positive_words if word in text.lower())
        neg_count = sum(1 for word in negative_words if word in text.lower())

        total = pos_count + neg_count
        if total == 0:
            return 0.0

        return (pos_count - neg_count) / total

    def _detect_language(self, text: str) -> str:
        """ç®€å•çš„è¯­è¨€æ£€æµ‹"""
        chinese_chars = len(re.findall(r"[\u4e00-\u9fff]", text))
        total_chars = len(text)

        if chinese_chars / total_chars > 0.1:
            return "zh"
        else:
            return "en"

    def get_supported_providers(self) -> List[NLPProvider]:
        """è·å–æ”¯æŒçš„ NLP æä¾›å•†"""
        providers = [NLPProvider.NONE]  # åŸºç¡€æ¨¡å¼æ€»æ˜¯æ”¯æŒ

        if HAS_FULL_NLP:
            if spacy:
                providers.append(NLPProvider.SPACY)
            if self._sentence_model:
                providers.append(NLPProvider.TRANSFORMERS)

        return providers

    async def warm_up_models(self, providers: List[NLPProvider]) -> None:
        """é¢„çƒ­ NLP æ¨¡å‹"""
        if not HAS_FULL_NLP:
            return

        for provider in providers:
            if provider == NLPProvider.SPACY:
                await self._preload_spacy_models()
            elif provider == NLPProvider.TRANSFORMERS and not self._sentence_model:
                # åŠ è½½ sentence-transformers æ¨¡å‹
                self._sentence_model = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: SentenceTransformer("all-MiniLM-L6-v2")
                )

    async def async_cleanup(self) -> None:
        """æ¸…ç† NLP èµ„æº"""
        self._spacy_models.clear()
        self._sentence_model = None
        logging.info("ğŸ§¹ NLP Service: Resources cleaned up")

    def _update_avg_processing_time(self, processing_time: float) -> None:
        """æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´ç»Ÿè®¡"""
        total_analyses = self.stats["total_analyses"]
        current_avg = self.stats["average_processing_time"]

        # æ»šåŠ¨å¹³å‡
        new_avg = (
            (current_avg * (total_analyses - 1)) + processing_time
        ) / total_analyses
        self.stats["average_processing_time"] = new_avg

    async def health_check(self) -> bool:
        """NLP æœåŠ¡å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•åŸºæœ¬æ–‡æœ¬åˆ†æåŠŸèƒ½
            test_result = await self.analyze_text("æµ‹è¯• test", "auto")
            return test_result.get("confidence", 0) > 0
        except Exception:
            return False


# å·¥å‚å‡½æ•°
def create_enhanced_nlp_service(
    config: Optional[NLPConfig] = None,
) -> EnhancedNLPService:
    """
    åˆ›å»ºå¢å¼ºç‰ˆ NLP æœåŠ¡å®ä¾‹

    Args:
        config: NLP é…ç½®å‚æ•°

    Returns:
        é…ç½®å¥½çš„å¢å¼ºç‰ˆ NLP æœåŠ¡
    """
    service = EnhancedNLPService(config)

    if HAS_FULL_NLP:
        logging.info("ğŸš€ Created EnhancedNLPService in FULL mode")
    else:
        logging.info("ğŸ“ Created EnhancedNLPService in BASIC mode")

    return service


if __name__ == "__main__":
    # æµ‹è¯•å¢å¼ºç‰ˆ NLP æœåŠ¡
    async def test_enhanced_nlp():
        service = create_enhanced_nlp_service()

        print("ğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆ NLP æœåŠ¡...")

        # åˆå§‹åŒ–æœåŠ¡
        await service.async_initialize()

        # å¥åº·æ£€æŸ¥
        health = await service.health_check()
        print(f"å¥åº·æ£€æŸ¥: {health}")

        if health:
            # æµ‹è¯•æ–‡æœ¬åˆ†æ
            test_texts = [
                "æ™ºèƒ½å¼€å…³æ§åˆ¶å™¨ï¼Œæ”¯æŒè¿œç¨‹å¼€å…³æ§åˆ¶",
                "æ¸©æ¹¿åº¦ä¼ æ„Ÿå™¨ï¼Œå®æ—¶ç›‘æµ‹ç¯å¢ƒæ•°æ®",
                "RGB LED light with brightness control",
            ]

            for text in test_texts:
                print(f"\nğŸ“ åˆ†ææ–‡æœ¬: {text}")
                result = await service.analyze_text(text, "auto")
                print(f"   è¯­è¨€: {result['language']}")
                print(f"   å…³é”®è¯: {result['keywords'][:5]}")
                print(f"   æƒ…æ„Ÿ: {result['sentiment']:.3f}")
                print(f"   ç½®ä¿¡åº¦: {result['confidence']:.3f}")
                print(f"   å¤„ç†æ¨¡å¼: {result.get('processing_mode', 'unknown')}")

            # æµ‹è¯•æ–‡æœ¬ç›¸ä¼¼åº¦
            print(f"\nğŸ” ç›¸ä¼¼åº¦æµ‹è¯•:")
            similarity = await service.compare_texts("æ™ºèƒ½å¼€å…³æ§åˆ¶", "å¼€å…³æ§åˆ¶å™¨")
            print(f"   ç›¸ä¼¼åº¦åˆ†æ•°: {similarity:.3f}")

            # æµ‹è¯•è®¾å¤‡ç‰¹å¾æå–
            print(f"\nğŸ  è®¾å¤‡ç‰¹å¾æå–:")
            features = await service.extract_device_features(
                "æ™ºèƒ½RGBç¯å…‰è°ƒèŠ‚å™¨ï¼Œæ”¯æŒäº®åº¦å’Œé¢œè‰²æ§åˆ¶"
            )
            print(f"   å¹³å°æç¤º: {features['platform_hints']}")
            print(f"   IOç±»å‹: {features['io_types']}")
            print(f"   ç½®ä¿¡åº¦: {features['confidence']:.3f}")

        await service.async_cleanup()
        print("\nâœ… æµ‹è¯•å®Œæˆ")

    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_enhanced_nlp())
